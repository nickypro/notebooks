{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb4d3a1fd00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -qqq circuitsvis && pip install -qqq -U torch sentence-transformers\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "from taker import Model\n",
    "from taker.hooks import HookConfig\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Model and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8966d45721fa4b3daa017ef5f305aefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'google/gemma-2-2b-it' with bfp16:\n",
      "- Added 416 hooks across 26 layers\n",
      " - n_layers : 26\n",
      " - d_model  : 2304\n",
      " - n_heads  : 8\n",
      " - d_head   : 256\n",
      " - d_mlp    : 9216\n"
     ]
    }
   ],
   "source": [
    "# m = Model(\"nickypro/tinyllama-15m\", dtype=\"bfp16\", compile=False, model_device=\"cuda\")\n",
    "# m = Model(\"microsoft/phi-3-mini-4k-instruct\", dtype=\"bfp16\", compile=False)\n",
    "m = Model(\"google/gemma-2-2b-it\", dtype=\"bfp16\", compile=False)\n",
    "m.show_details()\n",
    "\n",
    "has_double_newline_token = (m.get_ids(\"\\n\").shape == m.get_ids(\"\\n\\n\").shape)\n",
    "newline_token_id = m.get_ids(\"\\n\\n\")[0, -1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt    = \"\"\"Write a short blog post about a recipe and the inspiration behind it.\n",
    " Do not include a title.\n",
    " Only reveal the dish after the story.\n",
    " Start with short story and then move to the recipe.\n",
    " To re-iterate, do not include a title.\"\"\"\n",
    "# info_gen = m.generate(info_prompt, temperature=0.3, num=300)\n",
    "\n",
    "#DOUBLE NEWLINE\n",
    "info_gen = \"\"\"\n",
    "\\n Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
    "\n",
    "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
    "\n",
    "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\\n\\n\"\"\"\n",
    "info_prompt = prompt+info_gen\n",
    "\n",
    "info_ids = m.get_ids(info_prompt)\n",
    "info_embeds = m.get_inputs_embeds(info_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a neutral prompt for extraction. We try a randomised/scrambled prompt,\n",
    "and a fine-tuned prompt and see what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'Continuation', '▁of', '▁previous', '▁text', ':', '\\n\\n']\n",
      "[[4.15625   1.828125  1.8671875 1.71875   1.6796875 1.921875  2.34375  ]]\n",
      "[[4.15625   1.7734375 1.8046875 1.8203125 1.890625  1.8203125 2.34375  ]]\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Neutral Prompt\n",
    "neutral_prompt = \"Continuation of previous text:\\n\\n\"\n",
    "\n",
    "neutral_ids = m.get_ids(neutral_prompt)\n",
    "print(m.tokenizer.convert_ids_to_tokens(neutral_ids[0].tolist()))\n",
    "neutral_embeds = m.get_inputs_embeds(input_ids=neutral_ids)\n",
    "\n",
    "\n",
    "# Random inputs embeds\n",
    "def make_rand_embeds(neutral_embeds, start=1, end=5):\n",
    "    rand_embeds    = neutral_embeds.clone()\n",
    "    rand_embeds[0, start:end] = torch.randn_like(neutral_embeds[0, start:end]) / (m.cfg.d_model**0.5)\n",
    "    rand_embeds[0, start:end] *= neutral_embeds[0, start:end].norm(dim=-1).mean()\n",
    "\n",
    "    return rand_embeds\n",
    "\n",
    "rand_embeds = make_rand_embeds(neutral_embeds, 1, 6) # gemma\n",
    "# rand_embeds = make_rand_embeds(neutral_embeds, 0, 8) # phi3\n",
    "\n",
    "# Sanity check on norms\n",
    "print(neutral_embeds.norm(dim=-1).cpu().float().numpy())\n",
    "print(rand_embeds.norm(dim=-1).cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def write_to_file(experiment_name, data):\n",
    "    filename = f\"./results/{current_time}_story_agnes_{experiment_name}.jsonl\"\n",
    "    filename_latest = f\"./results/latest_story_agnes_{experiment_name}.jsonl\"\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for _filename in [filename, filename_latest]:\n",
    "        with open(_filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def read_file(experiment_name, time=\"latest\"):\n",
    "    filepath = f\"./results/{time}_agnes_story_{experiment_name}.jsonl\"\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "\n",
    "def reset_hooks():\n",
    "    #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "    [h.reset() for h in m.hooks.neuron_replace.values()]\n",
    "\n",
    "# Make tunable embed parameters:\n",
    "class TunableInputsEmbeds(torch.nn.Module):\n",
    "    def __init__(self, inputs_embeds):\n",
    "        super().__init__()\n",
    "        self.embeds = torch.nn.Parameter(inputs_embeds)\n",
    "        self.shape = self.embeds.shape\n",
    "\n",
    "    def forward(self):\n",
    "        return self.embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try generating some things already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get transferred activations and make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 6\n",
      "['▁family', \"'\", 's', '▁Sunday', '▁dinners', '.', '\\n\\n']\n",
      "['<bos>', 'Continuation', '▁of', '▁previous', '▁text', ':', '\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Get original text activations\n",
    "acts = m.get_midlayer_activations(info_prompt)\n",
    "orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "print(orig_token_index, new_token_index)\n",
    "print(m.tokenizer.convert_ids_to_tokens(m.get_ids(info_prompt).squeeze().tolist()[-neutral_ids.shape[1]:]))\n",
    "print(m.tokenizer.convert_ids_to_tokens(m.get_ids(neutral_prompt).squeeze().tolist()))\n",
    "\n",
    "def transfer_activations(num_tokens_transferred=1):\n",
    "    reset_hooks()\n",
    "    for j in range(num_tokens_transferred):\n",
    "        for layer_index in range(m.cfg.n_layers):\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index - j, acts[\"mlp\"][0, layer_index, orig_token_index - j])\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index - j, acts[\"attn\"][0, layer_index, orig_token_index - j])\n",
    "\n",
    "# Input parameters\n",
    "from dataclasses import dataclass\n",
    "model_repo = \"google/gemma-2b-it\"\n",
    "@dataclass\n",
    "class GenData:\n",
    "    model_repo: str = m.model_repo\n",
    "    temperature: float = 0.3\n",
    "    max_new_tokens: int = 100\n",
    "    tokens_transferred_num: int = 1\n",
    "    transplant_layers: tuple = (0,32)\n",
    "    num_tokens_transferred: int = 1\n",
    "    output: str = \"\"\n",
    "    curr_prompt: str = neutral_prompt\n",
    "    orig_prompt: str = info_prompt\n",
    "\n",
    "\n",
    "def generate_texts(data: GenData):\n",
    "    # Run generation with tranfer of 1 token\n",
    "    print(\"Generating texts with 1 token transferred...\")\n",
    "    for num_tokens_transferred in [1]:\n",
    "        data = GenData()\n",
    "        transfer_activations(data.num_tokens_transferred)\n",
    "        for i in range(5):\n",
    "            rand_embeds = make_rand_embeds(neutral_embeds)\n",
    "            embeds = TunableInputsEmbeds(rand_embeds)\n",
    "            for i in range(5):\n",
    "                text_in, text_out = m.generate(inputs_embeds=embeds(), num=data.max_new_tokens, temperature=data.temperature)\n",
    "                data.output = text_out\n",
    "                write_to_file(\"transfer-x1\", data.__dict__)\n",
    "\n",
    "    print(\"Generating texts from original info prompt...\")\n",
    "    # Run generation with tranfer of 1 token\n",
    "    data = GenData()\n",
    "    data.curr_prompt = info_prompt\n",
    "    reset_hooks()\n",
    "    for i in range(25):\n",
    "        text_in, text_out = m.generate(text=data.curr_prompt, num=data.max_new_tokens, temperature=data.temperature)\n",
    "        data.output = text_out\n",
    "        write_to_file(\"orig\", data.__dict__)\n",
    "\n",
    "    print(\"Generating texts from neutral prompt...\")\n",
    "    # Run generation with tranfer of 1 token\n",
    "    data = GenData()\n",
    "    data.curr_prompt = neutral_prompt\n",
    "    reset_hooks()\n",
    "    for i in range(25):\n",
    "        text_in, text_out = m.generate(text=data.curr_prompt, num=data.max_new_tokens, temperature=data.temperature)\n",
    "        data.output = text_out\n",
    "        write_to_file(\"neutral\", data.__dict__)\n",
    "\n",
    "# generate_texts(GenData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig': 'The dish was a vibrant and flavorful stew, bursting'}\n",
      "{'transfer': ' Monfieur\\n\\n\\n\\n'}\n",
      "{'no_transfer': ''}\n"
     ]
    }
   ],
   "source": [
    "# set torch nograd\n",
    "torch.set_grad_enabled(False)\n",
    "tuned_embeds = TunableInputsEmbeds(rand_embeds)\n",
    "\n",
    "def print_comparison():\n",
    "\n",
    "    # Get original text activations\n",
    "    reset_hooks()\n",
    "    acts = m.get_midlayer_activations(info_prompt)\n",
    "    orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "    new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "\n",
    "    def transfer_activations(num_tokens_transferred=1):\n",
    "        for j in range(num_tokens_transferred):\n",
    "            for layer_index in range(m.cfg.n_layers):\n",
    "                m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index - j, acts[\"mlp\"][0, layer_index, orig_token_index - j])\n",
    "                m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index - j, acts[\"attn\"][0, layer_index, orig_token_index - j])\n",
    "\n",
    "    @dataclass\n",
    "    class GenData:\n",
    "        model_repo: str = m.model_repo\n",
    "        temperature: float = 0.3\n",
    "        max_new_tokens: int = 20\n",
    "        num_tokens_transferred: int = 1\n",
    "        output: str = \"\"\n",
    "        curr_prompt: str = neutral_prompt\n",
    "        orig_prompt: str = info_prompt\n",
    "\n",
    "    def generate_text_with_tuned_embeds(data: GenData):\n",
    "        transfer_activations(data.num_tokens_transferred)\n",
    "        text_in, text_out = m.generate(inputs_embeds=tuned_embeds(), num=data.max_new_tokens, temperature=data.temperature)\n",
    "        return text_out\n",
    "\n",
    "    # For comparison, generate text from the original info prompt\n",
    "    reset_hooks()\n",
    "    text_in, text_out = m.generate(text=info_prompt, temperature=0.3)\n",
    "    print({\"orig\": text_out})\n",
    "\n",
    "    # Generate a single sample\n",
    "    reset_hooks()\n",
    "    text_out = generate_text_with_tuned_embeds(GenData())\n",
    "    print({\"transfer\": text_out})\n",
    "\n",
    "\n",
    "    # For comparison, generate text from the neutral prompt\n",
    "    reset_hooks()\n",
    "    text_in, text_out = m.generate(inputs_embeds=tuned_embeds(), temperature=0.3)\n",
    "    print({\"no_transfer\": text_out})\n",
    "\n",
    "print_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some comparison data, without any transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a method for loading up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Human: Write a detailed post-mortem analysis of a game development project you completed for a timed game development competition, structured as follows:\n",
      "\n",
      "- Begin with a brief introduction to the game, including a description and where it can be played.\n",
      "- Discuss the challenges and decisions made when working with the competition's theme, including any initial ideas that were eventually abandoned and the final concept chosen.\n",
      "- Describe your development setup and any notable tools or techniques used, such as livestreaming or specific programming frameworks.\n",
      "- Analyze the game's design, highlighting both successful and unsuccessful elements, including any usability issues encountered.\n",
      "- Discuss the development process, including any changes made along the way, features that were cut due to time constraints, and coding challenges faced.\n",
      "- Reflect on the game's overall user experience and any lessons learned regarding clarity and accessibility.\n",
      "- Conclude with a summary of the project's successes and failures, and any key takeaways for future game development projects, including any adjustments to scope and approach that would be made in retrospect.\n",
      "\n",
      "Assistant: \n",
      "\n",
      "Title: Post-Mortem Analysis: The Timed Adventure of \"Quest for the Lost Artifact\"\n",
      "\n",
      "Introduction:\n",
      "\"Quest for the Lost Artifact\" was a timed game development project that I completed for a competition. The game is an adventure puzzle set in a mystical world filled with ancient ruins and hidden treasures. Players must navigate through various levels, solve puzzles, and uncover the secrets of the lost artifact. The game can be played on multiple platforms, including PC, consoles, and mobile devices.\n",
      "\n",
      "\n",
      "--- BREAK POINT (Split index: 319) ---\n",
      "Challenges and Decisions:\n",
      "The competition's theme was \"Ancient Mysteries,\" which presented several challenges and decisions. Initially, I considered creating a game based on a mythical creature, but after researching the theme, I decided to focus on an ancient civilization. The final concept chosen was a game set in the lost city of Atlantis, where players must solve puzzles and uncover the secrets of the city's downfall.\n",
      "\n",
      "Development Setup:\n",
      "I used Unity as the primary game development engine, with C# for scripting. To streamline the development process, I utilized livestreaming on Twitch to share my progress with the community and receive feedback. Additionally, I used the Unity Asset Store to access pre-made assets, such as 3D models and sound effects, which helped speed up the development process.\n",
      "\n",
      "Game Design:\n",
      "The game's design was successful in creating an immersive and engaging experience for players. The puzzles were well-designed and challenging, requiring players to think critically and use logic to progress. However, there were some usability issues, such as the game's controls being too sensitive, causing players to accidentally trigger puzzle elements.\n",
      "\n",
      "Development Process:\n",
      "During the development process, I made several changes to the game's design, including simplifying some puzzles and adjusting\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n",
      "No split point found, skipping\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Initialize the tokenizer\n",
    "tokenizer = m.tokenizer\n",
    "\n",
    "def read_prompts():\n",
    "    with open(\"./results/latest_phi3_generations.jsonl\", \"r\") as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            full_text = data['full_text']\n",
    "\n",
    "            # Split the full text into prompt and output\n",
    "            prompt, output = full_text.split(\"Assistant:\", 1)\n",
    "            prompt += \"Assistant:\"  # Add back the \"Assistant:\" part\n",
    "\n",
    "            # Tokenize the full text and find the start of the output\n",
    "            full_tokens = tokenizer.encode(full_text)\n",
    "            output_start = len(tokenizer.encode(prompt))\n",
    "\n",
    "            # print(\"Input, Output Tokens:\", output_start, len(full_tokens))\n",
    "\n",
    "            # Find the index of \"\\n\\n\" after 100 tokens into the output\n",
    "            output_tokens = full_tokens[output_start:]\n",
    "            if len(output_tokens) > 100:\n",
    "                text_before_100_tokens = tokenizer.decode(output_tokens[:100])\n",
    "                text_after_100_tokens = tokenizer.decode(output_tokens[100:])\n",
    "                text_after_100_tokens_until_newline = text_after_100_tokens.split(\"\\n\\n\")[0]\n",
    "\n",
    "                if text_after_100_tokens_until_newline != text_after_100_tokens:\n",
    "                    full_index = tokenizer.encode(prompt + text_before_100_tokens + text_after_100_tokens_until_newline)\n",
    "                    data['split_index'] = len(full_index)\n",
    "                else:\n",
    "                    data['split_index'] = -1\n",
    "            else:\n",
    "                data['split_index'] = -1\n",
    "\n",
    "            if data['split_index'] == -1:\n",
    "                print(\"No split point found, skipping\")\n",
    "                continue\n",
    "\n",
    "            data[\"newline_index\"] = data[\"split_index\"] + int(not has_double_newline_token)\n",
    "\n",
    "            yield data\n",
    "\n",
    "# Example usage:\n",
    "for prompt_data in read_prompts():\n",
    "    full_text     = prompt_data['full_text']\n",
    "    newline_index = prompt_data['newline_index']\n",
    "\n",
    "    tokens = tokenizer.encode(full_text)\n",
    "    first_part  = tokenizer.decode(tokens[:newline_index+1])\n",
    "    second_part = tokenizer.decode(tokens[newline_index+1:])\n",
    "    print(f\"{first_part}\")\n",
    "    print(f\"--- BREAK POINT (Split index: {newline_index}) ---\")\n",
    "    print(f\"{second_part}\")\n",
    "\n",
    "    break\n",
    "\n",
    "prompts = list(read_prompts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norms before tensor([[4.1562, 1.7734, 1.8047, 1.8203, 1.8906, 1.8203, 2.3438]],\n",
      "       device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/273 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 25/273 [00:34<05:44,  1.39s/it, Avg Loss=1.5210]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norms after tensor([[4.9375, 3.2500, 3.2500, 3.2031, 3.2500, 3.2812, 3.6562]],\n",
      "       device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(True)  # Enable gradients for optimization\n",
    "\n",
    "# Initialize tuned_embeds as a TunableInputsEmbeds object\n",
    "tuned_embeds = TunableInputsEmbeds(rand_embeds)\n",
    "print(\"norms before\", tuned_embeds().norm(dim=-1))\n",
    "\n",
    "new_token_index = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "\n",
    "# Define loss function\n",
    "def get_ce_loss(expected_ids: torch.Tensor, logits: torch.Tensor):\n",
    "    \"\"\"Computes cross entropy losses for each token.\"\"\"\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    expected_ids = expected_ids.to(log_probs.device)\n",
    "    predicted_log_probs = log_probs.gather(dim=-1, index=expected_ids[..., None])[..., 0]\n",
    "    return -predicted_log_probs.mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(tuned_embeds.parameters(), lr=0.01)\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 8\n",
    "max_tokens = 100\n",
    "\n",
    "def process_sample(prompt_data):\n",
    "    full_ids = m.get_ids(prompt_data['full_text'])\n",
    "    orig_newline_index = prompt_data['newline_index']\n",
    "    ids_prompt = full_ids[:, :orig_newline_index+1]\n",
    "\n",
    "    # Validation check\n",
    "    newline_token_id = m.get_ids(\"\\n\\n\")[0, -1].item()\n",
    "    prompt_token_id = ids_prompt[0, -1].item()\n",
    "    if newline_token_id != prompt_token_id:\n",
    "        return None\n",
    "\n",
    "    info_output_ids = full_ids[0, orig_newline_index+1:orig_newline_index+1+max_tokens]\n",
    "\n",
    "    # Get original text activations\n",
    "    reset_hooks()\n",
    "    with torch.no_grad():\n",
    "        acts = m.get_midlayer_activations(input_ids=ids_prompt)\n",
    "        orig_acts = {\n",
    "            \"mlp\": acts[\"mlp\"][0, :, orig_newline_index],\n",
    "            \"attn\": acts[\"attn\"][0, :, orig_newline_index]\n",
    "        }\n",
    "\n",
    "    # Transfer activations\n",
    "    for layer_index in range(m.cfg.n_layers):\n",
    "        m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, orig_acts[\"mlp\"][layer_index])\n",
    "        m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, orig_acts[\"attn\"][layer_index])\n",
    "\n",
    "    # Forward pass\n",
    "    neutral_inputs = tuned_embeds()\n",
    "    info_output_embeds = m.get_inputs_embeds(input_ids=info_output_ids.unsqueeze(0))\n",
    "    neutral_embeds = torch.cat([neutral_inputs, info_output_embeds], dim=1)\n",
    "\n",
    "    logits = m.get_logits(inputs_embeds=neutral_embeds)\n",
    "    loss = get_ce_loss(info_output_ids.unsqueeze(0), logits[:, neutral_inputs.shape[1]-1:-1])\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "num_batches = (len(prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in (pbar := tqdm(range(num_batches))):\n",
    "    batch = prompts[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "\n",
    "    batch_loss = 0\n",
    "    valid_samples = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for prompt_data in batch:\n",
    "        loss = process_sample(prompt_data)\n",
    "\n",
    "        if loss is not None:\n",
    "            batch_loss += loss.item()\n",
    "            valid_samples += 1\n",
    "            loss.backward()\n",
    "\n",
    "    if valid_samples > 0:\n",
    "        # Update tuned_embeds after processing the entire batch\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = batch_loss / valid_samples\n",
    "        pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "    if batch_idx >= 25:  # Limit to 200 samples (25 batches of 8)\n",
    "        break\n",
    "\n",
    "# Generate text using the tuned embeddings\n",
    "print(\"norms after\", tuned_embeds().norm(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig': 'The recipe was a revelation. It was a hearty'}\n",
      "{'transfer': '<unused8>'}\n",
      "{'no_transfer': ''}\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "print_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
