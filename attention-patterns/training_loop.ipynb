{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa59b70ca60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -qqq circuitsvis && pip install -qqq -U torch sentence-transformers\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "from taker import Model\n",
    "from taker.hooks import HookConfig\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Model and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b67318ea8542d092c682419319fac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'google/gemma-2-2b-it' with bfp16:\n",
      "- Added 416 hooks across 26 layers\n",
      " - n_layers : 26\n",
      " - d_model  : 2304\n",
      " - n_heads  : 8\n",
      " - d_head   : 256\n",
      " - d_mlp    : 9216\n"
     ]
    }
   ],
   "source": [
    "# m = Model(\"nickypro/tinyllama-15m\", dtype=\"bfp16\", compile=False, model_device=\"cuda\")\n",
    "# m = Model(\"microsoft/phi-3-mini-4k-instruct\", dtype=\"bfp16\", compile=False)\n",
    "m = Model(\"google/gemma-2-2b-it\", dtype=\"bfp16\", compile=False)\n",
    "m.show_details()\n",
    "\n",
    "has_double_newline_token = (m.get_ids(\"\\n\").shape == m.get_ids(\"\\n\\n\").shape)\n",
    "newline_token_id = m.get_ids(\"\\n\\n\")[0, -1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt    = \"\"\"Write a short blog post about a recipe and the inspiration behind it.\n",
    " Do not include a title.\n",
    " Only reveal the dish after the story.\n",
    " Start with short story and then move to the recipe.\n",
    " To re-iterate, do not include a title.\"\"\"\n",
    "# info_gen = m.generate(info_prompt, temperature=0.3, num=300)\n",
    "\n",
    "#DOUBLE NEWLINE\n",
    "info_gen = \"\"\"\n",
    "\\n Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
    "\n",
    "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
    "\n",
    "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\\n\\n\"\"\"\n",
    "info_prompt = prompt+info_gen\n",
    "\n",
    "info_ids = m.get_ids(info_prompt)\n",
    "info_embeds = m.get_inputs_embeds(info_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a neutral prompt for extraction. We try a randomised/scrambled prompt,\n",
    "and a fine-tuned prompt and see what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'Continuation', '▁of', '▁previous', '▁text', ':', '\\n\\n']\n",
      "[[4.15625   1.828125  1.8671875 1.71875   1.6796875 1.921875  2.34375  ]]\n",
      "[[2.25     2.234375 2.203125 2.203125 2.1875   2.21875  2.21875 ]]\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Neutral Prompt\n",
    "neutral_prompt = \"Continuation of previous text:\\n\\n\"\n",
    "\n",
    "neutral_ids = m.get_ids(neutral_prompt)\n",
    "print(m.tokenizer.convert_ids_to_tokens(neutral_ids[0].tolist()))\n",
    "neutral_embeds = m.get_inputs_embeds(input_ids=neutral_ids)\n",
    "\n",
    "\n",
    "# Random inputs embeds\n",
    "def make_rand_embeds(neutral_embeds, start=1, end=5):\n",
    "    rand_embeds    = neutral_embeds.clone()\n",
    "    rand_embeds[0, start:end] = torch.randn_like(neutral_embeds[0, start:end]) / (m.cfg.d_model**0.5)\n",
    "    rand_embeds[0, start:end] *= neutral_embeds[0, start:end].norm(dim=-1).mean()\n",
    "\n",
    "    return rand_embeds\n",
    "\n",
    "# rand_embeds = make_rand_embeds(neutral_embeds, 1, 5) # gemma\n",
    "rand_embeds = make_rand_embeds(neutral_embeds, 0, 8) # phi3\n",
    "\n",
    "# Sanity check on norms\n",
    "print(neutral_embeds.norm(dim=-1).cpu().float().numpy())\n",
    "print(rand_embeds.norm(dim=-1).cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def write_to_file(experiment_name, data):\n",
    "    filename = f\"./results/{current_time}_story_agnes_{experiment_name}.jsonl\"\n",
    "    filename_latest = f\"./results/latest_story_agnes_{experiment_name}.jsonl\"\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for _filename in [filename, filename_latest]:\n",
    "        with open(_filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def read_file(experiment_name, time=\"latest\"):\n",
    "    filepath = f\"./results/{time}_agnes_story_{experiment_name}.jsonl\"\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "\n",
    "def reset_hooks():\n",
    "    #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "    [h.reset() for h in m.hooks.neuron_replace.values()]\n",
    "\n",
    "# Make tunable embed parameters:\n",
    "class TunableInputsEmbeds(torch.nn.Module):\n",
    "    def __init__(self, inputs_embeds):\n",
    "        super().__init__()\n",
    "        self.embeds = torch.nn.Parameter(inputs_embeds)\n",
    "        self.shape = self.embeds.shape\n",
    "\n",
    "    def forward(self):\n",
    "        return self.embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try generating some things already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get transferred activations and make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 6\n",
      "['▁family', \"'\", 's', '▁Sunday', '▁dinners', '.', '\\n\\n']\n",
      "['<bos>', 'Continuation', '▁of', '▁previous', '▁text', ':', '\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Get original text activations\n",
    "acts = m.get_midlayer_activations(info_prompt)\n",
    "orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "print(orig_token_index, new_token_index)\n",
    "print(m.tokenizer.convert_ids_to_tokens(m.get_ids(info_prompt).squeeze().tolist()[-neutral_ids.shape[1]:]))\n",
    "print(m.tokenizer.convert_ids_to_tokens(m.get_ids(neutral_prompt).squeeze().tolist()))\n",
    "\n",
    "def transfer_activations(num_tokens_transferred=1):\n",
    "    reset_hooks()\n",
    "    for j in range(num_tokens_transferred):\n",
    "        for layer_index in range(m.cfg.n_layers):\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index - j, acts[\"mlp\"][0, layer_index, orig_token_index - j])\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index - j, acts[\"attn\"][0, layer_index, orig_token_index - j])\n",
    "\n",
    "# Input parameters\n",
    "from dataclasses import dataclass\n",
    "model_repo = \"google/gemma-2b-it\"\n",
    "@dataclass\n",
    "class GenData:\n",
    "    model_repo: str = m.model_repo\n",
    "    temperature: float = 0.3\n",
    "    max_new_tokens: int = 100\n",
    "    tokens_transferred_num: int = 1\n",
    "    transplant_layers: tuple = (0,32)\n",
    "    num_tokens_transferred: int = 1\n",
    "    output: str = \"\"\n",
    "    curr_prompt: str = neutral_prompt\n",
    "    orig_prompt: str = info_prompt\n",
    "\n",
    "\n",
    "def generate_texts(data: GenData):\n",
    "    # Run generation with tranfer of 1 token\n",
    "    print(\"Generating texts with 1 token transferred...\")\n",
    "    for num_tokens_transferred in [1]:\n",
    "        data = GenData()\n",
    "        transfer_activations(data.num_tokens_transferred)\n",
    "        for i in range(5):\n",
    "            rand_embeds = make_rand_embeds(neutral_embeds)\n",
    "            embeds = TunableInputsEmbeds(rand_embeds)\n",
    "            for i in range(5):\n",
    "                text_in, text_out = m.generate(inputs_embeds=embeds(), num=data.max_new_tokens, temperature=data.temperature)\n",
    "                data.output = text_out\n",
    "                write_to_file(\"transfer-x1\", data.__dict__)\n",
    "\n",
    "    print(\"Generating texts from original info prompt...\")\n",
    "    # Run generation with tranfer of 1 token\n",
    "    data = GenData()\n",
    "    data.curr_prompt = info_prompt\n",
    "    reset_hooks()\n",
    "    for i in range(25):\n",
    "        text_in, text_out = m.generate(text=data.curr_prompt, num=data.max_new_tokens, temperature=data.temperature)\n",
    "        data.output = text_out\n",
    "        write_to_file(\"orig\", data.__dict__)\n",
    "\n",
    "    print(\"Generating texts from neutral prompt...\")\n",
    "    # Run generation with tranfer of 1 token\n",
    "    data = GenData()\n",
    "    data.curr_prompt = neutral_prompt\n",
    "    reset_hooks()\n",
    "    for i in range(25):\n",
    "        text_in, text_out = m.generate(text=data.curr_prompt, num=data.max_new_tokens, temperature=data.temperature)\n",
    "        data.output = text_out\n",
    "        write_to_file(\"neutral\", data.__dict__)\n",
    "\n",
    "# generate_texts(GenData())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some comparison data, without any transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a method for loading up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Human: Write a detailed post-mortem analysis of a game development project you completed for a timed game development competition, structured as follows:\n",
      "\n",
      "- Begin with a brief introduction to the game, including a description and where it can be played.\n",
      "- Discuss the challenges and decisions made when working with the competition's theme, including any initial ideas that were eventually abandoned and the final concept chosen.\n",
      "- Describe your development setup and any notable tools or techniques used, such as livestreaming or specific programming frameworks.\n",
      "- Analyze the game's design, highlighting both successful and unsuccessful elements, including any usability issues encountered.\n",
      "- Discuss the development process, including any changes made along the way, features that were cut due to time constraints, and coding challenges faced.\n",
      "- Reflect on the game's overall user experience and any lessons learned regarding clarity and accessibility.\n",
      "- Conclude with a summary of the project's successes and failures, and any key takeaways for future game development projects, including any adjustments to scope and approach that would be made in retrospect.\n",
      "\n",
      "Assistant: \n",
      "\n",
      "Title: Post-Mortem Analysis: The Timed Adventure of \"Quest for the Lost Artifact\"\n",
      "\n",
      "Introduction:\n",
      "\"Quest for the Lost Artifact\" was a timed game development project that I completed for a competition. The game is an adventure puzzle set in a mystical world filled with ancient ruins and hidden treasures. Players must navigate through various levels, solve puzzles, and uncover the secrets of the lost artifact. The game can be played on multiple platforms, including PC, consoles, and mobile devices.\n",
      "\n",
      "\n",
      "--- BREAK POINT (Split index: 319) ---\n",
      "Challenges and Decisions:\n",
      "The competition's theme was \"Ancient Mysteries,\" which presented several challenges and decisions. Initially, I considered creating a game based on a mythical creature, but after researching the theme, I decided to focus on an ancient civilization. The final concept chosen was a game set in the lost city of Atlantis, where players must solve puzzles and uncover the secrets of the city's downfall.\n",
      "\n",
      "Development Setup:\n",
      "I used Unity as the primary game development engine, with C# for scripting. To streamline the development process, I utilized livestreaming on Twitch to share my progress with the community and receive feedback. Additionally, I used the Unity Asset Store to access pre-made assets, such as 3D models and sound effects, which helped speed up the development process.\n",
      "\n",
      "Game Design:\n",
      "The game's design was successful in creating an immersive and engaging experience for players. The puzzles were well-designed and challenging, requiring players to think critically and use logic to progress. However, there were some usability issues, such as the game's controls being too sensitive, causing players to accidentally trigger puzzle elements.\n",
      "\n",
      "Development Process:\n",
      "During the development process, I made several changes to the game's design, including simplifying some puzzles and adjusting\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Initialize the tokenizer\n",
    "tokenizer = m.tokenizer\n",
    "\n",
    "def read_prompts():\n",
    "    with open(\"./results/latest_phi3_generations.jsonl\", \"r\") as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            full_text = data['full_text']\n",
    "\n",
    "            # Split the full text into prompt and output\n",
    "            prompt, output = full_text.split(\"Assistant:\", 1)\n",
    "            prompt += \"Assistant:\"  # Add back the \"Assistant:\" part\n",
    "\n",
    "            # Tokenize the full text and find the start of the output\n",
    "            full_tokens = tokenizer.encode(full_text)\n",
    "            output_start = len(tokenizer.encode(prompt))\n",
    "\n",
    "            # print(\"Input, Output Tokens:\", output_start, len(full_tokens))\n",
    "\n",
    "            # Find the index of \"\\n\\n\" after 100 tokens into the output\n",
    "            output_tokens = full_tokens[output_start:]\n",
    "            if len(output_tokens) > 100:\n",
    "                text_before_100_tokens = tokenizer.decode(output_tokens[:100])\n",
    "                text_after_100_tokens = tokenizer.decode(output_tokens[100:])\n",
    "                text_after_100_tokens_until_newline = text_after_100_tokens.split(\"\\n\\n\")[0]\n",
    "\n",
    "                if text_after_100_tokens_until_newline != text_after_100_tokens:\n",
    "                    full_index = tokenizer.encode(prompt + text_before_100_tokens + text_after_100_tokens_until_newline)\n",
    "                    data['split_index'] = len(full_index)\n",
    "                else:\n",
    "                    data['split_index'] = -1\n",
    "            else:\n",
    "                data['split_index'] = -1\n",
    "\n",
    "            if data['split_index'] == -1:\n",
    "                print(\"No split point found, skipping\")\n",
    "                continue\n",
    "\n",
    "            data[\"newline_index\"] = data[\"split_index\"] + int(not has_double_newline_token)\n",
    "\n",
    "            yield data\n",
    "\n",
    "# Example usage:\n",
    "for prompt_data in read_prompts():\n",
    "    full_text     = prompt_data['full_text']\n",
    "    newline_index = prompt_data['newline_index']\n",
    "\n",
    "    tokens = tokenizer.encode(full_text)\n",
    "    first_part  = tokenizer.decode(tokens[:newline_index+1])\n",
    "    second_part = tokenizer.decode(tokens[newline_index+1:])\n",
    "    print(f\"{first_part}\")\n",
    "    print(f\"--- BREAK POINT (Split index: {newline_index}) ---\")\n",
    "    print(f\"{second_part}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.54it/s, Loss=19.5000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No split point found, skipping\n",
      "{'idx': 1, 'len': 247, 'orig_len': 473, 'tail end': \" with VisionaryTech's SmartLens.\\n\\n\\n\\n\"}\n",
      "{'idx': 2, 'len': 207, 'orig_len': 419, 'tail end': ', and create lasting memories with fellow anglers.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.86it/s, Loss=20.2500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 6, 'len': 303, 'orig_len': 472, 'tail end': ' the foundation for a healthy, independent mindset.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:01,  7.58it/s, Loss=20.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 10, 'len': 322, 'orig_len': 511, 'tail end': ' CAD, ultimately improving patient care and outcomes.\\n\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:02,  6.96it/s, Loss=15.3125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 16, 'len': 291, 'orig_len': 459, 'tail end': ' the need for non-invasive monitoring techniques.\\n\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:04,  6.81it/s, Loss=13.8125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 24, 'len': 352, 'orig_len': 522, 'tail end': ' but also patients and the broader healthcare community.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:04,  7.23it/s, Loss=13.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 28, 'len': 230, 'orig_len': 461, 'tail end': ' to modify, resulting in a segmentation fault.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:05,  5.58it/s, Loss=12.3750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 34, 'len': 292, 'orig_len': 472, 'tail end': ' that invites further exploration into its enduring relevance.\\n\\n\\n'}\n",
      "{'idx': 35, 'len': 328, 'orig_len': 523, 'tail end': ' more efficient and cost-effective financial services.\\n\\n\\n'}\n",
      "{'idx': 36, 'len': 311, 'orig_len': 455, 'tail end': ' fail to integrate seamlessly with existing financial software.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:05,  9.94it/s, Loss=12.5000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 38, 'len': 260, 'orig_len': 445, 'tail end': ' and manipulated in a consistent and reliable manner.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:07,  6.78it/s, Loss=12.5625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 49, 'len': 318, 'orig_len': 450, 'tail end': ' measurement of disease burden and response to therapy.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:08,  7.45it/s, Loss=11.9375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 52, 'len': 241, 'orig_len': 446, 'tail end': \" treatment for women's mental health concerns.\\n\\n\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:08,  7.39it/s, Loss=12.2500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 56, 'len': 432, 'orig_len': 484, 'tail end': ' as much as its giver or provider intended.\\n\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:10,  6.73it/s, Loss=11.3750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 68, 'len': 224, 'orig_len': 469, 'tail end': 'letter sequence: \"LMN OP QR\"\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:11,  7.21it/s, Loss=11.5000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 72, 'len': 199, 'orig_len': 410, 'tail end': ' a research fellow and later an associate professor.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:13,  6.73it/s, Loss=11.8750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 83, 'len': 254, 'orig_len': 432, 'tail end': ' offering a range of shopping and dining options.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [00:13,  7.41it/s, Loss=11.3125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 86, 'len': 228, 'orig_len': 447, 'tail end': '-step process to securely delete your emails:\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:14,  6.25it/s, Loss=11.1875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No split point found, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [00:14,  7.07it/s, Loss=nan]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 93, 'len': 272, 'orig_len': 446, 'tail end': ' could potentially alter the outcome of the case.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:15,  7.66it/s, Loss=11.3125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 96, 'len': 219, 'orig_len': 402, 'tail end': ' the zero-crossing point of the waveform.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:17,  6.69it/s, Loss=11.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 109, 'len': 282, 'orig_len': 437, 'tail end': ' catheter management to reduce the risk of complications.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [00:18,  7.16it/s, Loss=10.0625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 113, 'len': 169, 'orig_len': 378, 'tail end': ' resolved, and she made a full recovery.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:20,  6.77it/s, Loss=10.0625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 123, 'len': 254, 'orig_len': 455, 'tail end': ' address the socioeconomic determinants of T2DM.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:23,  6.71it/s, Loss=10.6250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 142, 'len': 185, 'orig_len': 399, 'tail end': ' to maintain sharpness and reduce wear over time.\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [00:27,  6.58it/s, Loss=10.3125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 164, 'len': 208, 'orig_len': 446, 'tail end': ' our website or contacting a local agent today.\\n\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169it [00:28,  7.45it/s, Loss=11.3750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 167, 'len': 303, 'orig_len': 524, 'tail end': 'lassified under the family \"Brownaceae.\"\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178it [00:29,  6.77it/s, Loss=9.8750] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 176, 'len': 301, 'orig_len': 481, 'tail end': ' (approx. 1-2 sentences).\\n\\n\\n\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [00:31,  5.21it/s, Loss=9.3125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 187, 'len': 460, 'orig_len': 497, 'tail end': \"Helvetica'; fontSize = 14;>\\n\\n\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197it [00:32,  6.69it/s, Loss=10.3125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 195, 'len': 268, 'orig_len': 456, 'tail end': '-term outcomes in terms of patient comfort.\\n\\n\\n'}\n",
      "No split point found, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:33,  5.93it/s, Loss=9.9375] \n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)  # Enable gradients for optimization\n",
    "\n",
    "# Initialize rand_embeds as a TunableInputsEmbeds object\n",
    "# rand_embeds = make_rand_embeds(neutral_embeds)\n",
    "tuned_embeds = TunableInputsEmbeds(rand_embeds)\n",
    "new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "\n",
    "# Define an optimizer for rand_embeds\n",
    "def get_ce_loss(expected_ids: torch.Tensor, logits: torch.Tensor):\n",
    "    \"\"\"Computes cross entropy losses for each token.\"\"\"\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    expected_ids = expected_ids.to(log_probs.device)\n",
    "    predicted_log_probs = log_probs.gather(dim=-1, index=expected_ids[..., None])[..., 0]\n",
    "    return -predicted_log_probs.mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(tuned_embeds.parameters(), lr=0.001)\n",
    "\n",
    "for idx, prompt_data in (pbar := tqdm(enumerate(read_prompts()))):\n",
    "    # Read the text\n",
    "    full_ids = m.get_ids(prompt_data['full_text'])\n",
    "    orig_newline_index = prompt_data['newline_index']\n",
    "    ids_prompt = full_ids[:, :orig_newline_index+1]\n",
    "    try:\n",
    "        newline_token_id = m.get_ids(\"\\n\\n\")[0, -1].item()\n",
    "        prompt_token_id = ids_prompt[0, -1].item()\n",
    "        assert newline_token_id == prompt_token_id, f\"Final token is not a newline token: {m.tokenizer.decode(prompt_token_id)}\"\n",
    "    except AssertionError as e:\n",
    "        # print(e)\n",
    "        print({\"idx\": idx, \"len\": len(ids_prompt[0]), \"orig_len\": len(full_ids[0]), \"tail end\": tokenizer.decode(ids_prompt[0].tolist()[-10:])})\n",
    "        continue\n",
    "\n",
    "    # Get info prompt input\n",
    "    info_output_ids = full_ids[:, newline_index+1:][:, :100] # limit to 100 tokens\n",
    "\n",
    "    # Get info prompt output\n",
    "    info_output_embeds = m.get_inputs_embeds(input_ids=info_output_ids)\n",
    "\n",
    "    # Get original text input activations\n",
    "    reset_hooks()\n",
    "    with torch.no_grad():\n",
    "        # Get original text activations\n",
    "        acts = m.get_midlayer_activations(input_ids=ids_prompt)\n",
    "        orig_token_index = ids_prompt.shape[1] - 1\n",
    "        orig_acts = {\n",
    "            \"mlp\" : acts[\"mlp\"][0, :, orig_newline_index],\n",
    "            \"attn\": acts[\"attn\"][0, :, orig_newline_index]\n",
    "        }\n",
    "\n",
    "    # transfer activations\n",
    "    for layer_index in range(m.cfg.n_layers):\n",
    "        m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, orig_acts[\"mlp\"][layer_index])\n",
    "        m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, orig_acts[\"attn\"][layer_index])\n",
    "\n",
    "    # Begin training on this text\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    neutral_inputs = tuned_embeds()\n",
    "    neutral_outputs = info_output_embeds\n",
    "    neutral_embeds = torch.cat([neutral_inputs, neutral_outputs], dim=1)\n",
    "\n",
    "    # Forward pass with rand_embed\n",
    "    logits = m.get_logits(inputs_embeds=neutral_embeds)\n",
    "    loss   = get_ce_loss(info_output_ids, logits[..., neutral_inputs.shape[-1]:-1])\n",
    "\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update rand_embeds\n",
    "\n",
    "    # print(f'{idx}: Loss: {loss.item():.4f}')\n",
    "    # update tqdm description within the loading bar\n",
    "    pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "    # print(loss.item())\n",
    "\n",
    "    if idx >= 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('',\n",
       " ' itſelf \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n  \\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n ')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get original text input activations\n",
    "reset_hooks()\n",
    "with torch.no_grad():\n",
    "    # Get original text activations\n",
    "    acts = m.get_midlayer_activations(input_ids=ids_prompt)\n",
    "    orig_token_index = ids_prompt.shape[1] - 1\n",
    "    orig_acts = {\n",
    "        \"mlp\" : acts[\"mlp\"][0, :, orig_newline_index],\n",
    "        \"attn\": acts[\"attn\"][0, :, orig_newline_index]\n",
    "    }\n",
    "\n",
    "# transfer activations\n",
    "for layer_index in range(m.cfg.n_layers):\n",
    "    m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, orig_acts[\"mlp\"][layer_index])\n",
    "    m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, orig_acts[\"attn\"][layer_index])\n",
    "\n",
    "m.generate(inputs_embeds=tuned_embeds(), num=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
