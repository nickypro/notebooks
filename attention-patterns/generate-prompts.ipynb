{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prompts based on the Pile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "import json\n",
    "import numpy as np\n",
    "from huggingface_hub import InferenceClient\n",
    "from taker.texts import prepare\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = np.random.randint(0, 1e7)\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# write prompts to a file\n",
    "def write_to_file(data):\n",
    "    filename = f\"./results/{current_time}_prompts.jsonl\"\n",
    "    filename_latest = f\"./results/latest_prompts.jsonl\"\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for _filename in [filename, filename_latest]:\n",
    "        with open(_filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "# Store current data here\n",
    "@dataclass\n",
    "class GenPrompts:\n",
    "    # model = \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "    model:str = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "    seed:int = seed\n",
    "    max_new_tokens:int = 420\n",
    "    dataset_repo:str = \"pile\"\n",
    "    index:int = 0\n",
    "    completion:str = \"\"\n",
    "\n",
    "cfg = GenPrompts()\n",
    "\n",
    "\n",
    "# Write a function for getting written completions\n",
    "client = InferenceClient(cfg.model)\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=cfg.max_new_tokens,\n",
    "        stream=False,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# Write a function for getting structured prompts\n",
    "get_prompt = lambda __text : f\"\"\"\n",
    "{__text[:10000]}\n",
    "\n",
    "---\n",
    "\n",
    "Please write a structured question that asks to write something that resembles\n",
    "the content written above. Do not give overly specific names, but do give details\n",
    "on how the text should be structured. Keep it short\n",
    "\n",
    "Do not NOT write \"Sure, here it is\" or similar. Just write the question.\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset, label, _ = prepare(cfg.dataset_repo)\n",
    "num_to_skip = 36\n",
    "\n",
    "for i, data in enumerate(dataset):\n",
    "    if i < num_to_skip:\n",
    "        continue\n",
    "    text = data[label]\n",
    "    prompt = get_prompt(text)\n",
    "    cfg.index = i\n",
    "    cfg.completion = get_completion(prompt)\n",
    "    write_to_file(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate generations based on the prompts from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     64\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m generation\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     66\u001b[0m     output_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_prompt,\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_text,\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\n\u001b[1;32m     70\u001b[0m     }\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m     write_generation(output_data)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerations saved to ./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_phi3_generations.jsonl and ./results/latest_phi3_generations.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Stop"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "import json\n",
    "import numpy as np\n",
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = np.random.randint(0, 1e7)\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "@dataclass\n",
    "class GenPrompts:\n",
    "    model: str = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "    seed: int = seed\n",
    "    max_new_tokens: int = 420\n",
    "    dataset_repo: str = \"pile\"\n",
    "    index: int = 0\n",
    "    completion: str = \"\"\n",
    "\n",
    "cfg = GenPrompts()\n",
    "\n",
    "# Update the client\n",
    "client = InferenceClient(cfg.model)\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = client.text_generation(\n",
    "        prompt,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        return_full_text=True,\n",
    "        stream=False,\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# Function to read prompts from latest_prompts.jsonl\n",
    "def read_prompts():\n",
    "    with open(\"./results/latest_prompts.jsonl\", \"r\") as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "# Function to write generations to files\n",
    "def write_generation(data):\n",
    "    filenames = [\n",
    "        filename := f\"./results/{current_time}_phi3_generations.jsonl\",\n",
    "        filename_latest := \"./results/latest_phi3_generations.jsonl\"\n",
    "    ]\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"a\") as file:\n",
    "            json.dump(data, file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "# Generate outputs for each prompt\n",
    "for prompt_data in tqdm(read_prompts()):\n",
    "    input_prompt = prompt_data[\"completion\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    full_prompt = f\"Human: {input_prompt}\\n\\nAssistant: \"\n",
    "\n",
    "    generation = get_completion(full_prompt)\n",
    "    #Â Get only the output part of the text:\n",
    "    output_text = generation.split(\"Assistant: \")[1]\n",
    "\n",
    "    output_data = {\n",
    "        \"input\": input_prompt,\n",
    "        \"output\": output_text,\n",
    "        \"full_text\": generation\n",
    "    }\n",
    "\n",
    "    write_generation(output_data)\n",
    "\n",
    "print(f\"Generations saved to ./results/{current_time}_phi3_generations.jsonl and ./results/latest_phi3_generations.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
