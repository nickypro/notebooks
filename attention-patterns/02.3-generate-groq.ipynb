{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n",
      "100%|██████████| 50/50 [02:09<00:00,  2.58s/it]:03<09:32, 63.58s/it]\n",
      "100%|██████████| 50/50 [02:08<00:00,  2.57s/it]:12<13:36, 102.08s/it]\n",
      "100%|██████████| 50/50 [02:07<00:00,  2.54s/it]:21<13:19, 114.24s/it]\n",
      "100%|██████████| 50/50 [02:07<00:00,  2.55s/it]:28<11:56, 119.35s/it]\n",
      "100%|██████████| 50/50 [02:09<00:00,  2.59s/it]:35<10:11, 122.27s/it]\n",
      "100%|██████████| 50/50 [02:10<00:00,  2.60s/it]:45<08:18, 124.73s/it]\n",
      "100%|██████████| 50/50 [02:07<00:00,  2.54s/it]:55<06:19, 126.47s/it]\n",
      "100%|██████████| 50/50 [02:08<00:00,  2.58s/it]:02<04:13, 126.70s/it]\n",
      "100%|██████████| 50/50 [02:07<00:00,  2.55s/it]:11<02:07, 127.36s/it]\n",
      "Generating responses: 100%|██████████| 10/10 [20:18<00:00, 121.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "# Load prompts from file\n",
    "version = \"V2\"\n",
    "num_copies = 10\n",
    "SKIP_TEXTS = 0\n",
    "with open(f\"./data/{version}_prompts.txt\", \"r\") as f:\n",
    "    prompts = f.read().splitlines()\n",
    "\n",
    "# Function to format input for Gemma 2 9B\n",
    "def format_input(prompt):\n",
    "    return f\"<bos><start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>assistant\\n\"\n",
    "\n",
    "# Generate responses and save to JSONL\n",
    "with open(f\"./data/{version}_orig_generation.jsonl\", \"a\") as outfile:\n",
    "    for prompt in tqdm(prompts[SKIP_TEXTS:], desc=\"Generating responses\"):\n",
    "        for i in tqdm(range(num_copies)):\n",
    "            formatted_input = format_input(prompt)\n",
    "\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"gemma2-9b-it\",\n",
    "            )\n",
    "\n",
    "            response = chat_completion.choices[0].message.content\n",
    "            formatted_full_text = formatted_input + response\n",
    "\n",
    "            result = {\n",
    "                \"text\": prompt,\n",
    "                \"output\": response,\n",
    "                \"formatted_input\": formatted_input,\n",
    "                \"formatted_full_text\": formatted_full_text\n",
    "            }\n",
    "\n",
    "            json.dump(result, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(\"Generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
