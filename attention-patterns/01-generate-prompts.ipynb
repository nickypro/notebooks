{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prompts based on the Pile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "import json\n",
    "import numpy as np\n",
    "from huggingface_hub import InferenceClient\n",
    "from taker.texts import prepare\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = np.random.randint(0, 1e7)\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# write prompts to a file\n",
    "def write_to_file(data):\n",
    "    filename = f\"./results/{current_time}_prompts.jsonl\"\n",
    "    filename_latest = f\"./results/latest_prompts.jsonl\"\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for _filename in [filename, filename_latest]:\n",
    "        with open(_filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "# Store current data here\n",
    "@dataclass\n",
    "class GenPrompts:\n",
    "    # model = \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "    model:str = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "    seed:int = seed\n",
    "    max_new_tokens:int = 420\n",
    "    dataset_repo:str = \"pile\"\n",
    "    index:int = 0\n",
    "    completion:str = \"\"\n",
    "\n",
    "cfg = GenPrompts()\n",
    "\n",
    "\n",
    "# Write a function for getting written completions\n",
    "client = InferenceClient(cfg.model)\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=cfg.max_new_tokens,\n",
    "        stream=False,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# Write a function for getting structured prompts\n",
    "get_prompt = lambda __text : f\"\"\"\n",
    "{__text[:10000]}\n",
    "\n",
    "---\n",
    "\n",
    "Please write a structured question that asks to write something that resembles\n",
    "the content written above. Do not give overly specific names, but do give details\n",
    "on how the text should be structured. Keep it short\n",
    "\n",
    "Do not NOT write \"Sure, here it is\" or similar. Just write the question.\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset, label, _ = prepare(cfg.dataset_repo)\n",
    "num_to_skip = 36\n",
    "\n",
    "for i, data in enumerate(dataset):\n",
    "    if i < num_to_skip:\n",
    "        continue\n",
    "    text = data[label]\n",
    "    prompt = get_prompt(text)\n",
    "    cfg.index = i\n",
    "    cfg.completion = get_completion(prompt)\n",
    "    write_to_file(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate generations based on the prompts from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [02:00, ?it/s]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2-9b-it (Request ID: -I5eK_7arw2Pa5gSnyzsP)\n\nModel too busy, unable to get response in less than 120 second(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2-9b-it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m input_prompt \u001b[38;5;241m=\u001b[39m prompt_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     61\u001b[0m full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 63\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#Â Get only the output part of the text:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m output_text \u001b[38;5;241m=\u001b[39m generation\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt):\n\u001b[0;32m---> 28\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2061\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2037\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   2040\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[1;32m   2060\u001b[0m         )\n\u001b[0;32m-> 2061\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_common.py:460\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2032\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2032\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2034\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2-9b-it (Request ID: -I5eK_7arw2Pa5gSnyzsP)\n\nModel too busy, unable to get response in less than 120 second(s)"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "import json\n",
    "import numpy as np\n",
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = np.random.randint(0, 1e7)\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "@dataclass\n",
    "class GenPrompts:\n",
    "    # model: str = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "    model: str = \"google/gemma-2-9b-it\"\n",
    "    seed: int = seed\n",
    "    max_new_tokens: int = 420\n",
    "    dataset_repo: str = \"pile\"\n",
    "    index: int = 0\n",
    "    completion: str = \"\"\n",
    "\n",
    "cfg = GenPrompts()\n",
    "\n",
    "# Update the client\n",
    "client = InferenceClient(cfg.model)\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = client.text_generation(\n",
    "        prompt,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        return_full_text=True,\n",
    "        stream=False,\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# Function to read prompts from latest_prompts.jsonl\n",
    "def read_prompts():\n",
    "    with open(\"./results/latest_prompts.jsonl\", \"r\") as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "# Function to write generations to files\n",
    "def write_generation(data):\n",
    "    filenames = [\n",
    "        filename := f\"./results/{current_time}_gemma2_generations.jsonl\",\n",
    "        filename_latest := \"./results/latest_gemma2_generations.jsonl\"\n",
    "    ]\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"a\") as file:\n",
    "            json.dump(data, file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "# Generate outputs for each prompt\n",
    "for prompt_data in tqdm(read_prompts()):\n",
    "    input_prompt = prompt_data[\"completion\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    full_prompt = f\"Human: {input_prompt}\\n\\nAssistant: \"\n",
    "\n",
    "    generation = get_completion(full_prompt)\n",
    "    #Â Get only the output part of the text:\n",
    "    output_text = generation.split(\"Assistant: \")[1]\n",
    "\n",
    "    output_data = {\n",
    "        \"input\": input_prompt,\n",
    "        \"output\": output_text,\n",
    "        \"full_text\": generation\n",
    "    }\n",
    "\n",
    "    write_generation(output_data)\n",
    "\n",
    "print(f\"Generations saved to ./results/{current_time}_gemma2_generations.jsonl and ./results/latest_gemma2_generations.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
