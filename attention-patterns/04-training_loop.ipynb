{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qqq circuitsvis && pip install -qqq -U torch sentence-transformers\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "from taker import Model\n",
    "from taker.hooks import HookConfig\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Model and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Model(\"nickypro/tinyllama-15m\", dtype=\"bfp16\", compile=False, model_device=\"cuda\")\n",
    "m = Model(\"google/gemma-2b-it\", dtype=\"hqq8\", compile=True)\n",
    "# m = Model(\"microsoft/phi-3-mini-4k-instruct\", dtype=\"hqq8\", compile=True)\n",
    "m.show_details()\n",
    "\n",
    "has_double_newline_token = (m.get_ids(\"\\n\").shape == m.get_ids(\"\\n\\n\").shape)\n",
    "newline_token_id = m.get_ids(\"\\n\\n\")[0, -1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt    = \"\"\"Write a short blog post about a recipe and the inspiration behind it.\n",
    " Do not include a title.\n",
    " Only reveal the dish after the story.\n",
    " Start with short story and then move to the recipe.\n",
    " To re-iterate, do not include a title.\"\"\"\n",
    "# info_gen = m.generate(info_prompt, temperature=0.3, num=300)\n",
    "\n",
    "#DOUBLE NEWLINE\n",
    "info_gen = \"\"\"\n",
    "\\n Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
    "\n",
    "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
    "\n",
    "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\\n\\n\"\"\"\n",
    "info_prompt = prompt+info_gen\n",
    "\n",
    "info_ids = m.get_ids(info_prompt)\n",
    "info_embeds = m.get_inputs_embeds(info_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a neutral prompt for extraction. We try a randomised/scrambled prompt,\n",
    "and a fine-tuned prompt and see what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Neutral Prompt\n",
    "neutral_prompt = \"------------------------------------------------\\n\\n\"\n",
    "\n",
    "neutral_ids = m.get_ids(neutral_prompt)\n",
    "print(m.tokenizer.convert_ids_to_tokens(neutral_ids[0].tolist()))\n",
    "neutral_embeds = m.get_inputs_embeds(input_ids=neutral_ids)\n",
    "\n",
    "\n",
    "# Random inputs embeds\n",
    "def make_rand_embeds(neutral_embeds, start=1, end=2):\n",
    "    rand_embeds    = neutral_embeds.clone()\n",
    "    rand_embeds[0, start:end] = torch.randn_like(neutral_embeds[0, start:end]) / (m.cfg.d_model**0.5)\n",
    "    rand_embeds[0, start:end] *= neutral_embeds[0, start:end].norm(dim=-1).mean()\n",
    "\n",
    "    return rand_embeds\n",
    "\n",
    "# rand_embeds = make_rand_embeds(neutral_embeds, 1, 6) # gemma\n",
    "# rand_embeds = make_rand_embeds(neutral_embeds, 0, 8) # phi3\n",
    "rand_embeds = make_rand_embeds(neutral_embeds, 1, 4) # gemma ---\n",
    "\n",
    "# Sanity check on norms\n",
    "print(neutral_embeds.norm(dim=-1).cpu().float().numpy())\n",
    "print(rand_embeds.norm(dim=-1).cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def write_to_file(experiment_name, data):\n",
    "    filename = f\"./results/{current_time}_story_agnes_{experiment_name}.jsonl\"\n",
    "    filename_latest = f\"./results/latest_story_agnes_{experiment_name}.jsonl\"\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        with open(filename_latest, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    for _filename in [filename, filename_latest]:\n",
    "        with open(_filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def read_file(experiment_name, time=\"latest\"):\n",
    "    filepath = f\"./results/{time}_agnes_story_{experiment_name}.jsonl\"\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "\n",
    "def reset_hooks():\n",
    "    #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "    [h.reset() for h in m.hooks.neuron_replace.values()]\n",
    "\n",
    "# Make tunable embed parameters:\n",
    "class TunableInputsEmbeds(torch.nn.Module):\n",
    "    def __init__(self, inputs_embeds):\n",
    "        super().__init__()\n",
    "        self.embeds = torch.nn.Parameter(inputs_embeds)\n",
    "        self.shape = self.embeds.shape\n",
    "\n",
    "    def forward(self):\n",
    "        return self.embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try generating some things already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get transferred activations and make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original text activations\n",
    "acts = m.get_midlayer_activations(info_prompt)\n",
    "orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "print(orig_token_index, new_token_index)\n",
    "print(m.tokenizer.convert_ids_to_tokens(m.get_ids(info_prompt).squeeze().tolist()[-neutral_ids.shape[1]:]))\n",
    "print(m.tokenizer.convert_ids_to_tokens(m.get_ids(neutral_prompt).squeeze().tolist()))\n",
    "\n",
    "def transfer_activations(num_tokens_transferred=1):\n",
    "    reset_hooks()\n",
    "    for j in range(num_tokens_transferred):\n",
    "        for layer_index in range(m.cfg.n_layers):\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index - j, acts[\"mlp\"][0, layer_index, orig_token_index - j])\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index - j, acts[\"attn\"][0, layer_index, orig_token_index - j])\n",
    "\n",
    "# Input parameters\n",
    "from dataclasses import dataclass\n",
    "model_repo = \"google/gemma-2b-it\"\n",
    "@dataclass\n",
    "class GenData:\n",
    "    model_repo: str = m.model_repo\n",
    "    temperature: float = 0.3\n",
    "    max_new_tokens: int = 20\n",
    "    tokens_transferred_num: int = 1\n",
    "    transplant_layers: tuple = (0,32)\n",
    "    num_tokens_transferred: int = 1\n",
    "    output: str = \"\"\n",
    "    curr_prompt: str = neutral_prompt\n",
    "    orig_prompt: str = info_prompt\n",
    "\n",
    "\n",
    "def generate_texts(data: GenData, tuned_embeds=None):\n",
    "    # Run generation with tranfer of 1 token\n",
    "    print({\"orig token\": m.tokenizer.decode(m.get_ids(info_prompt)[0, orig_token_index]), \"transfer token\": m.tokenizer.decode(m.get_ids(neutral_prompt)[0, new_token_index])})\n",
    "    for num_tokens_transferred in [1]:\n",
    "        data = GenData()\n",
    "        transfer_activations(data.num_tokens_transferred)\n",
    "        for i in range(5):\n",
    "            if tuned_embeds is None:\n",
    "                rand_embeds = make_rand_embeds(neutral_embeds)\n",
    "                embeds = TunableInputsEmbeds(rand_embeds)\n",
    "            else:\n",
    "                embeds = tuned_embeds\n",
    "            for i in range(5):\n",
    "                text_in, text_out = m.generate(inputs_embeds=embeds(), num=data.max_new_tokens, temperature=data.temperature)\n",
    "                data.output = text_out\n",
    "                write_to_file(\"transfer-x1\", data.__dict__)\n",
    "\n",
    "    print(\"Generating texts from original info prompt...\")\n",
    "    # Run generation with tranfer of 1 token\n",
    "    data = GenData()\n",
    "    data.curr_prompt = info_prompt\n",
    "    reset_hooks()\n",
    "    for i in range(25):\n",
    "        text_in, text_out = m.generate(text=data.curr_prompt, num=data.max_new_tokens, temperature=data.temperature)\n",
    "        data.output = text_out\n",
    "        write_to_file(\"orig\", data.__dict__)\n",
    "\n",
    "    print(\"Generating texts from neutral prompt...\")\n",
    "    # Run generation with tranfer of 1 token\n",
    "    data = GenData()\n",
    "    data.curr_prompt = neutral_prompt\n",
    "    reset_hooks()\n",
    "    for i in range(25):\n",
    "        text_in, text_out = m.generate(text=data.curr_prompt, num=data.max_new_tokens, temperature=data.temperature)\n",
    "        data.output = text_out\n",
    "        write_to_file(\"neutral\", data.__dict__)\n",
    "\n",
    "# generate_texts(GenData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch nograd\n",
    "torch.set_grad_enabled(False)\n",
    "tuned_embeds = TunableInputsEmbeds(rand_embeds)\n",
    "\n",
    "def generate(m, inputs_embeds):\n",
    "    generate_ids = m.predictor.generate(inputs_embeds=inputs_embeds, max_length=12,\n",
    "        do_sample=True, temperature=0.3)\n",
    "    text_after  = m.tokenizer.batch_decode( generate_ids,\n",
    "        skip_special_tokens=True, clean_up_tokenization_spaces=False )[0]\n",
    "    return \"\", text_after\n",
    "\n",
    "def print_comparison():\n",
    "\n",
    "    # Get original text activations\n",
    "    reset_hooks()\n",
    "    acts = m.get_midlayer_activations(info_prompt)\n",
    "    orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "    new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "    print({\"orig token\": m.tokenizer.decode(m.get_ids(info_prompt)[0, orig_token_index]), \"transfer token\": m.tokenizer.decode(m.get_ids(neutral_prompt)[0, new_token_index])})\n",
    "\n",
    "    def transfer_activations(num_tokens_transferred=1):\n",
    "        for j in range(num_tokens_transferred):\n",
    "            for layer_index in range(m.cfg.n_layers):\n",
    "                m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index - j, 1*acts[\"mlp\"][0, layer_index, orig_token_index - j])\n",
    "                m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index - j, 1*acts[\"attn\"][0, layer_index, orig_token_index - j])\n",
    "\n",
    "    @dataclass\n",
    "    class GenData:\n",
    "        model_repo: str = m.model_repo\n",
    "        temperature: float = 0.3\n",
    "        max_new_tokens: int = 20\n",
    "        num_tokens_transferred: int = 1\n",
    "        output: str = \"\"\n",
    "        curr_prompt: str = neutral_prompt\n",
    "        orig_prompt: str = info_prompt\n",
    "\n",
    "    def generate_text_with_tuned_embeds(data: GenData):\n",
    "        transfer_activations(data.num_tokens_transferred)\n",
    "        print(tuned_embeds().shape)\n",
    "        text_in, text_out = generate(m, tuned_embeds())\n",
    "        # text_in, text_out = m.generate(inputs_embeds=tuned_embeds(), num=data.max_new_tokens, temperature=data.temperature)\n",
    "        return text_in, text_out\n",
    "\n",
    "    # For comparison, generate text from the original info prompt\n",
    "    reset_hooks()\n",
    "    text_in, text_out = m.generate(text=info_prompt, temperature=0.3)\n",
    "    print({\"orig\": text_out})\n",
    "\n",
    "    # Generate a single sample\n",
    "    reset_hooks()\n",
    "    text_in, text_out = generate_text_with_tuned_embeds(GenData())\n",
    "    print({\"transfer\": text_out})\n",
    "\n",
    "\n",
    "    # For comparison, generate text from the neutral prompt\n",
    "    reset_hooks()\n",
    "    text_in, text_out = generate(m, tuned_embeds())\n",
    "    print({\"no_transfer\": text_out})\n",
    "\n",
    "print_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some comparison data, without any transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a method for loading up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Initialize the tokenizer\n",
    "tokenizer = m.tokenizer\n",
    "\n",
    "def read_prompts(path=\"./results/latest_phi3_generations.jsonl\"):\n",
    "    with open(path, \"r\") as file:\n",
    "        invalid_count = 0\n",
    "        for line in (pbar:=tqdm(file)):\n",
    "            data = json.loads(line)\n",
    "            full_text = data['full_text']\n",
    "\n",
    "            # Split the full text into prompt and output\n",
    "            prompt, output = full_text.split(\"Assistant:\", 1)\n",
    "            prompt += \"Assistant:\"  # Add back the \"Assistant:\" part\n",
    "\n",
    "            # Tokenize the full text and find the start of the output\n",
    "            full_tokens = m.tokenizer.encode(full_text)\n",
    "            output_start = len(m.tokenizer.encode(prompt))\n",
    "\n",
    "            # print(\"Input, Output Tokens:\", output_start, len(full_tokens))\n",
    "\n",
    "            # Find the index of \"\\n\\n\" after 100 tokens into the output\n",
    "            output_tokens = full_tokens[output_start:]\n",
    "            if len(output_tokens) > 100:\n",
    "                text_before_100_tokens = m.tokenizer.decode(output_tokens[:100])\n",
    "                text_after_100_tokens = m.tokenizer.decode(output_tokens[100:])\n",
    "                text_after_100_tokens_until_newline = text_after_100_tokens.split(\"\\n\\n\")[0]\n",
    "\n",
    "                if text_after_100_tokens_until_newline != text_after_100_tokens:\n",
    "                    full_index = m.tokenizer.encode(prompt + text_before_100_tokens + text_after_100_tokens_until_newline)\n",
    "                    data['split_index'] = len(full_index)\n",
    "                else:\n",
    "                    data['split_index'] = -1\n",
    "            else:\n",
    "                data['split_index'] = -1\n",
    "\n",
    "            if data['split_index'] == -1:\n",
    "                invalid_count += 1\n",
    "                # update pbar with invalid count.\n",
    "                pbar.set_description(f\"Invalid prompts: {invalid_count}\")\n",
    "                continue\n",
    "\n",
    "            data[\"newline_index\"] = data[\"split_index\"]\n",
    "\n",
    "            yield data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "for prompt_data in read_prompts():\n",
    "    full_text     = prompt_data['full_text']\n",
    "    newline_index = prompt_data['newline_index']\n",
    "\n",
    "    tokens = tokenizer.encode(full_text)\n",
    "    first_part  = tokenizer.decode(tokens[:newline_index+1])\n",
    "    second_part = tokenizer.decode(tokens[newline_index+1:])\n",
    "    # print(f\"{first_part}\")\n",
    "    # print(f\"--- BREAK POINT (Split index: {newline_index}) ---\")\n",
    "    # print(f\"{second_part}\")\n",
    "\n",
    "    break\n",
    "\n",
    "prompts = list(read_prompts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_grad_enabled(True)  # Enable gradients for optimization\n",
    "\n",
    "# Initialize tuned_embeds as a TunableInputsEmbeds object\n",
    "tuned_embeds = TunableInputsEmbeds(rand_embeds)\n",
    "print(\"norms before\", tuned_embeds().norm(dim=-1))\n",
    "\n",
    "new_token_index = m.get_ids(neutral_prompt).shape[1] - 1\n",
    "\n",
    "# Define KL divergence loss function\n",
    "def kl_divergence_loss(baseline_logits, output_logits):\n",
    "    baseline_probs = F.softmax(baseline_logits, dim=-1)\n",
    "    output_log_probs = F.log_softmax(output_logits, dim=-1)\n",
    "    kl_div = F.kl_div(output_log_probs, baseline_probs, reduction='none', log_target=False).sum()\n",
    "    return kl_div\n",
    "\n",
    "optimizer = torch.optim.Adam(tuned_embeds.parameters(), lr=0.001)\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 10\n",
    "max_tokens = 100\n",
    "invalid_samples = 0\n",
    "\n",
    "def process_sample(prompt_data):\n",
    "    full_ids = m.get_ids(prompt_data['full_text'])\n",
    "    orig_newline_index = prompt_data['newline_index']\n",
    "    ids_prompt = full_ids[:, :orig_newline_index+1]\n",
    "\n",
    "    # Validation check\n",
    "    newline_token_id = m.get_ids(\"\\n\\n\")[0, -1].item()\n",
    "    prompt_token_id = ids_prompt[0, -1].item()\n",
    "    if newline_token_id != prompt_token_id:\n",
    "        return None\n",
    "\n",
    "    # Ensure we don't exceed the available tokens\n",
    "    available_tokens = full_ids.shape[1] - (orig_newline_index + 1)\n",
    "    tokens_to_use = min(available_tokens, max_tokens)\n",
    "\n",
    "    o_start, o_end = orig_newline_index+1, orig_newline_index+1+tokens_to_use\n",
    "    baseline_ids = full_ids[:, o_start:o_end]\n",
    "\n",
    "    # Get original text activations\n",
    "    reset_hooks()\n",
    "    with torch.no_grad():\n",
    "        acts = m.get_midlayer_activations(input_ids=ids_prompt)\n",
    "        orig_acts = {\n",
    "            \"mlp\": acts[\"mlp\"][0, :, orig_newline_index],\n",
    "            \"attn\": acts[\"attn\"][0, :, orig_newline_index]\n",
    "        }\n",
    "\n",
    "    # Transfer activations\n",
    "    for layer_index in range(m.cfg.n_layers):\n",
    "        m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, orig_acts[\"mlp\"][layer_index])\n",
    "        m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, orig_acts[\"attn\"][layer_index])\n",
    "\n",
    "    # Forward pass for baseline\n",
    "    with torch.no_grad():\n",
    "        baseline_logits = m.get_logits(input_ids=full_ids)[:, o_start:o_end]\n",
    "\n",
    "    # Forward pass for output\n",
    "    expected_tokens = m.get_inputs_embeds(input_ids=baseline_ids)\n",
    "    inputs_embeds = torch.cat([tuned_embeds()[:, :-1], tuned_embeds()[:, -1:].detach(), expected_tokens], dim=1)\n",
    "    output_logits = m.get_logits(inputs_embeds=inputs_embeds)[:, tuned_embeds.shape[1]-1:-1]\n",
    "\n",
    "    loss = kl_divergence_loss(baseline_logits, output_logits)\n",
    "\n",
    "    return loss\n",
    "# Training loop\n",
    "num_batches = (len(prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in (pbar := tqdm(range(num_batches))):\n",
    "    batch = prompts[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "\n",
    "    batch_loss = 0 #torch.tensor(0.0, device=tuned_embeds.embeds.device, requires_grad=True)\n",
    "    valid_samples = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for prompt_data in batch:\n",
    "        loss = process_sample(prompt_data)\n",
    "\n",
    "        if loss is not None:\n",
    "            batch_loss = batch_loss + loss\n",
    "            valid_samples += 1\n",
    "        else:\n",
    "            invalid_samples += 1\n",
    "\n",
    "    if batch_loss > 0:\n",
    "        batch_loss.backward()\n",
    "\n",
    "    if valid_samples > 0:\n",
    "        # Update tuned_embeds after processing the entire batch\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = batch_loss.item() / valid_samples\n",
    "        pbar.set_postfix({'Avg KL Div': f'{avg_loss:.4f}', 'Skipped': f'{invalid_samples}'})\n",
    "\n",
    "    if batch_idx >= 100:\n",
    "        break\n",
    "\n",
    "# Generate text using the tuned embeddings\n",
    "print(\"norms after\", tuned_embeds().norm(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "print_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ids = m.get_ids(prompt_data['full_text'])\n",
    "orig_newline_index = prompt_data['newline_index']\n",
    "ids_prompt = full_ids[:, :orig_newline_index+1]\n",
    "print(full_ids[:, orig_newline_index+1:orig_newline_index+1+100])\n",
    "print({\"out\": m.tokenizer.decode( full_ids[0, orig_newline_index+1:orig_newline_index+1+100].tolist() )})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
